# 学习教程（RAG检索增强生成）

本教程帮助你系统学习项目的目标、结构、依赖、设计与核心流程，并带你进行代码导读与实践建议。确保与当前项目代码保持一致与准确。

## 项目介绍

- 目标：实现一个可运行的 RAG（检索增强生成）系统，包含数据入库（索引构建）、语义检索与结合上下文的回答生成。
- 特点：模块化清晰、依赖可替换、具备降级运行能力（无外部服务时仍可运行）。

## 目录结构（当前实际）

```
RAG检索增强生成/
├── README.md
├── requirements.txt
├── .env.example
├── app/
│   ├── __init__.py
│   ├── config.py
│   ├── pipelines/
│   │   ├── __init__.py
│   │   ├── ingest.py
│   │   ├── retrieve.py
│   │   └── generate.py
│   ├── services/
│   │   ├── embeddings.py
│   │   ├── vector_store.py
│   │   └── llm.py
│   └── utils/
│       ├── doc_loader.py
│       ├── text_splitter.py
│       └── logging.py
├── data/
│   └── .gitkeep
└── scripts/
    ├── ingest.py
    └── query.py
```

## 依赖与工具（用途准确）

- `python-dotenv`：从 `.env` 加载配置（路径、切分、模型选择、向量库等）。
- `numpy`：数值计算与检索降级（在 FAISS 不可用时使用）。
- `sentence-transformers`：本地嵌入模型（默认）。
- `faiss-cpu`：高效向量检索（可选，若不可用自动降级到 NumPy）。
- `openai`：LLM 生成（在 `llm.py` 中使用；未配置时使用本地回退生成）。
- 标准库 `logging`：统一日志输出（`utils/logging.py`）。

以项目 `requirements.txt` 为安装来源；在 Windows 环境下 `faiss-cpu` 可能安装失败，代码已具备降级逻辑。

## 设计与架构

- 配置层（`app/config.py`）：统一加载与校验运行参数；含数据路径、切分、模型与向量库选择。
- 工具层（`app/utils/`）：
  - `doc_loader.py`：读取 `.txt`/`.md` 文件为统一的文本输入。
  - `text_splitter.py`：按字符切分文本，支持 `chunk_size` 与 `chunk_overlap`。
  - `logging.py`：统一日志格式与级别。
- 服务层（`app/services/`）：
  - `embeddings.py`：嵌入生成（Sentence-Transformers，失败时哈希伪向量降级）。
  - `vector_store.py`：向量库（FAISS 优先；不可用时使用 NumPy 余弦相似度），支持保存/加载。
  - `llm.py`：回答生成（OpenAI 可选；未配置时使用“检索拼接”回退）。
- 管道层（`app/pipelines/`）：
  - `ingest.py`：入库管道，将文档“加载→切分→嵌入→入库”，元数据包含源路径与内容。
  - `retrieve.py`：检索管道，将查询向量化后检索并返回最相关的内容块。
  - `generate.py`：生成管道，封装从检索上下文到答案生成的过程。
- 脚本层（`scripts/`）：
  - `ingest.py`：命令行入库工具。
  - `query.py`：命令行查询工具（支持仅检索或检索+生成）。

## 流程详解

- 入库（索引构建）：
  1. 读取 `source_dir` 下的 `.txt`/`.md` 文档（`doc_loader.py`）。
  2. 按 `chunk_size` 与 `chunk_overlap` 切分为块（`text_splitter.py`）。
  3. 将每个块转为向量（`embeddings.py`）。
  4. 构建向量索引并保存（`vector_store.py`）。
- 检索：
  1. 将查询向量化（与索引同模型）。
  2. 在向量库中进行相似度检索（FAISS/NumPy）。
  3. 返回最相关的内容块与元数据。
- 生成：
  1. 将检索到的上下文与问题输入到 LLM（`llm.py`）。
  2. 返回最终回答（未配置外部 LLM 时，使用回退拼接回答）。

## 代码导读（按文件）

- `app/config.py`：`AppConfig`、`load_config`；了解默认值与环境覆盖。
- `app/utils/doc_loader.py`：安全读取文本文件并返回内容与路径。
- `app/utils/text_splitter.py`：切分参数的边界检查与切分策略。
- `app/services/embeddings.py`：嵌入实现与降级路径。
- `app/services/vector_store.py`：索引保存/加载、检索逻辑与回退。
- `app/services/llm.py`：生成实现与回退逻辑。
- `app/pipelines/ingest.py`、`retrieve.py`、`generate.py`：端到端管道组合。
- `scripts/ingest.py`、`scripts/query.py`：CLI 参数解析与调用。

## 学习建议与实践

- 从 3–5 个短文本开始，调节 `chunk_size`/`chunk_overlap` 观察检索质量变化。
- 对比不同嵌入模型的速度与效果（轻量 vs 更强模型）。
- 若环境不支持 `faiss-cpu`，使用内置降级检索，先跑通流程。
- 将答案同时输出来源 `metadata['source']`，便于验证与迭代。

## 常见问题与排查

- FAISS 安装失败：使用降级检索（无需改代码），或在后续切换到 Chroma（需自行扩展）。
- OpenAI 未配置：可使用仅检索，或使用项目的回退生成；不影响索引与检索。
- 检索命中差：增大 `chunk_size`、适度提高 `chunk_overlap`；选择更强的嵌入模型；清洗重复和噪声文本。

---

如需增加 Web API（FastAPI）或评测脚本（召回率、引用覆盖率），可在本项目基础上扩展。